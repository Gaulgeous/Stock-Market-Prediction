{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import InputLayer, LSTM, Dense, Conv1D, Flatten, GRU, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, mean_squared_error as mse, mean_absolute_percentage_error as mape, mean_absolute_error as mae\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras import regularizers\n",
    "# from tpot import TPOTRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "import absl.logging\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "from sklearn import metrics\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import statistics\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import keras_tuner as kt\n",
    "from pandas_datareader import data as pdr\n",
    "from datetime import date, timedelta\n",
    "from copy import deepcopy\n",
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "timeframe = 9000\n",
    "enable_pca = 0\n",
    "standard_scaling = 0\n",
    "win_size = 5\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lower = 0.7\n",
    "upper = 0.8\n",
    "future_window = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we have a multi-index: let's collapse that so we have usable, single index column names\n",
    "def collapse_columns(df):\n",
    "    df = df.copy()\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.to_series().apply(lambda x: \"__\".join(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "def set_verbosity():\n",
    "    absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "    tf.compat.v1.logging.set_verbosity(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame(days, stock):   \n",
    "    end = date.today()\n",
    "    start = end - timedelta(days=days)\n",
    "    yf.pdr_override()\n",
    "\n",
    "    data = yf.download(stock, start, end)\n",
    "\n",
    "\n",
    "    data = data.resample('D').first() # ALWAYS resample before shifting so we don't get the wrong shift amount if there are missing rows/timestamps\n",
    "    data = collapse_columns(data)\n",
    "    data = data.dropna(how='any', axis='rows')\n",
    "\n",
    "    assert data.isna().any().any() == False # Make sure there are no NaNs left\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's alot of multi-collinearity in this data. Ideally, we should remove colinear features, as they will \n",
    "# Skew results\n",
    "# After calling this function, simply remove these correlated columns from the dataset (Better to not have any of them)\n",
    "# PCA is another option for removing it\n",
    "\n",
    "def remove_correlation(data, threshold):\n",
    "    correlated_cols = set()\n",
    "    correlation_matrix = data.corr()\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix[i, j]) > threshold:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                correlated_cols.add(colname)\n",
    "\n",
    "    return correlated_cols\n",
    "\n",
    "\n",
    "def remove_correlations_PCA(X):\n",
    "\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    pca = PCA().fit_transform(X_std)\n",
    "\n",
    "    # Use these two indicators to see which variables are having the most effect on the system\n",
    "    # Choose the high few impacts, and put them into the new PCA\n",
    "    print(np.cumsum(pca.explained_variance_ratio))\n",
    "    print(pca.explained_variance_ratio)\n",
    "\n",
    "    # Change num_componenets to be the number of useful variables observed above\n",
    "    pca = PCA(num_components=1).fit_transform(X_std)\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is the final part of the preprocessing pipeline, and is used to remove columns that are unnecessary\n",
    "class FeatureDropper(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        X.drop(['Volume', 'Adj Close'], axis=1, inplace=True, errors='ignore')\n",
    "        if enable_pca:\n",
    "            X = remove_correlations_PCA(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "# This class is the final part of the preprocessing pipeline, and is used to remove columns that are unnecessary\n",
    "class FeatureScaler(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        if standard_scaling:\n",
    "            open = StandardScaler(feature_range=(0, 1))\n",
    "            high = StandardScaler(feature_range=(0, 1))\n",
    "            low = StandardScaler(feature_range=(0, 1))\n",
    "            close = StandardScaler(feature_range=(0, 1))\n",
    "\n",
    "        else:\n",
    "            open = MinMaxScaler(feature_range=(0, 1))\n",
    "            high = MinMaxScaler(feature_range=(0, 1))\n",
    "            low = MinMaxScaler(feature_range=(0, 1))\n",
    "            close = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "        X['Open'] = open.fit_transform(X[['Open']])\n",
    "        X['High'] = high.fit_transform(X[['High']])\n",
    "        X['Low'] = low.fit_transform(X[['Low']])\n",
    "        X['Close'] = close.fit_transform(X[['Close']])\n",
    "        \n",
    "        return X, open, high, low, close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_tpot(data, future_window, win_size):\n",
    "    \n",
    "    np_data = data.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    future_X = []\n",
    "    for i in range(len(np_data)-(win_size+future_window)):\n",
    "        row = [r for r in np_data[i:i+win_size]]\n",
    "        X.append(list(np.concatenate(row).flat))\n",
    "        label = np_data[i+win_size+future_window][3]\n",
    "        y.append(label)\n",
    "\n",
    "    for i in range(len(np_data) - win_size):\n",
    "        row = [r for r in np_data[i:i+win_size]]\n",
    "        future_X.append(list(np.concatenate(row).flat))\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(future_X)\n",
    "\n",
    "\n",
    "def create_dataset(data, future_window, win_size):\n",
    "    \n",
    "    np_data = data.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    future_X = []\n",
    "    for i in range(len(np_data)-(win_size+future_window)):\n",
    "        row = [r for r in np_data[i:i+win_size]]\n",
    "        X.append(row)\n",
    "        label = np_data[i+win_size+future_window]\n",
    "        y.append(label)\n",
    "\n",
    "    for i in range(len(np_data) - win_size):\n",
    "        row = [r for r in np_data[i:i+win_size]]\n",
    "        future_X.append(row)\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(future_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kt_model(hp):\n",
    "\n",
    "    pipe = Pipeline([('Dropper', FeatureDropper()), ('Scaler', FeatureScaler())])\n",
    "    frame = load_frame(timeframe, 'AAPL')\n",
    "    frame, open, high, low, close = pipe.fit_transform(frame)\n",
    "    X, y, future_X = create_dataset(frame, future_window, win_size)\n",
    "\n",
    "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
    "    hp_learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    hp_reg = hp.Float(\"reg\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    hp_dropout = hp.Float(\"dropout\", min_value=1e-3, max_value=0.5, sampling=\"linear\")\n",
    "    hp_neuron_pct = hp.Float('NeuronPct', min_value=1e-3, max_value=1.0, sampling='linear')\n",
    "    hp_neuron_shrink = hp.Float('NeuronShrink', min_value=1e-3, max_value=1.0, sampling='linear')\n",
    "    \n",
    "    hp_l_layer_1 = hp.Int('l_layer_1', min_value=1, max_value=100, step=10)\n",
    "    hp_l_layer_2 = hp.Int('l_layer_2', min_value=1, max_value=100, step=10)\n",
    "    hp_max_neurons = hp.Int('neurons', min_value=10, max_value=200, step=10)\n",
    "\n",
    "    neuron_count = int(hp_neuron_pct * hp_max_neurons)\n",
    "    layers = 0\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer((X.shape[1], X.shape[2])))\n",
    "    model.add(LSTM(hp_l_layer_1, return_sequences=True, activity_regularizer=regularizers.l1(hp_reg)))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    model.add(LSTM(hp_l_layer_2, return_sequences=True, activity_regularizer=regularizers.l1(hp_reg)))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    while neuron_count > 5 and layers < 5:\n",
    "\n",
    "        model.add(Dense(units=neuron_count, activation=hp_activation))\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        layers += 1\n",
    "        neuron_count = int(neuron_count * hp_neuron_shrink)\n",
    "\n",
    "    model.add(Dense(4, 'linear'))\n",
    "\n",
    "    model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=hp_learning_rate), \n",
    "                metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(models, cache, inverters, stock):\n",
    "\n",
    "    cols = ['Open', 'High', 'Low', 'Close']\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if name == 'model_7':\n",
    "            window = 7\n",
    "        elif name == 'model_30':\n",
    "            window = 30\n",
    "        else:\n",
    "            window = 90\n",
    "\n",
    "        dates = list(cache.get('data').index)\n",
    "        last = dates[-1]\n",
    "        for i in range(window):\n",
    "                dates.append(last + pd.DateOffset(days=i+1))\n",
    "        dates = dates[window+5:]\n",
    "        output = pd.DataFrame(data={'Dates':dates})\n",
    "        \n",
    "        preds = model.predict(cache.get(name))\n",
    "        for pred in range(4):\n",
    "            output[cols[pred]] = inverters[pred].inverse_transform(preds[:,pred].reshape(-1, 1))\n",
    "            \n",
    "        output = output.set_index('Dates')\n",
    "        output.to_csv('stocks_csvs/' + name + '_' + stock + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistence model\n",
    "def model_persistence(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_baselines(frame, future_window, stock):\n",
    "\n",
    "    original_Open = frame[['Open']].copy()\n",
    "    shifted_Open = frame[['Open']].shift(-future_window)\n",
    "    frame['Open+window'] = shifted_Open\n",
    "\n",
    "    original_Close = frame[['Close']].copy()\n",
    "    shifted_Close = frame[['Close']].shift(-future_window)\n",
    "    frame['Close+window'] = shifted_Close\n",
    "\n",
    "    original_High = frame[['High']].copy()\n",
    "    shifted_High = frame[['High']].shift(-future_window)\n",
    "    frame['High+window'] = shifted_High\n",
    "\n",
    "    original_Low = frame[['Low']].copy()\n",
    "    shifted_Low = frame[['Low']].shift(-future_window)\n",
    "    frame['Low+window'] = shifted_Low\n",
    "        \n",
    "    frame.dropna()\n",
    "    frame.to_csv('stocks_csvs/' + str(stock) + '_persistance_' + str(future_window) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_persistence(frame, future_window):\n",
    "\n",
    "    original_Open = frame[['Open']].copy()\n",
    "    shifted_Open = frame[['Open']].shift(-future_window)\n",
    "    frame['Open+window'] = shifted_Open\n",
    "\n",
    "    original_Close = frame[['Close']].copy()\n",
    "    shifted_Close = frame[['Close']].shift(-future_window)\n",
    "    frame['Close+window'] = shifted_Close\n",
    "\n",
    "    original_High = frame[['High']].copy()\n",
    "    shifted_High = frame[['High']].shift(-future_window)\n",
    "    frame['High+window'] = shifted_High\n",
    "\n",
    "    original_Low = frame[['Low']].copy()\n",
    "    shifted_Low = frame[['Low']].shift(-future_window)\n",
    "    frame['Low+window'] = shifted_Low\n",
    "        \n",
    "    frame = frame.dropna(how='any', axis='rows')\n",
    "\n",
    "    predictions = np.array(frame.drop(['Open', 'High', 'Low', 'Close'], axis=1, errors='ignore'))\n",
    "    y = np.array(frame.drop(['Open+window', 'High+window', 'Low+window', 'Close+window'], axis=1, errors='ignore'))\n",
    "\n",
    "    r2 = r2_score(y, predictions)\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    rmse = mean_squared_error(y, predictions,  squared=False)\n",
    "    mape = mean_absolute_percentage_error(y, predictions)\n",
    "    mae = mean_absolute_error(y, predictions)\n",
    "\n",
    "    performance = pd.DataFrame(data={'metric': ['RMSE', 'MSE', 'MAPE', 'MAE', 'R2'], 'value': [rmse, mse, mape, mae, r2]})\n",
    "    performance.to_csv('stocks_csvs/baseline_performance_' + str(future_window) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outputs(stocks):\n",
    "\n",
    "    model_7 = load_model('models/model_7')\n",
    "    model_30 = load_model('models/model_30')\n",
    "    model_90 = load_model('models/model_90')\n",
    "    models = {'model_7': model_7, 'model_30': model_30, 'model_90': model_90}\n",
    "\n",
    "    pipe = Pipeline([('Dropper', FeatureDropper()), ('Scaler', FeatureScaler())])\n",
    "    end = date.today()\n",
    "    start = end - timedelta(days=timeframe)\n",
    "    yf.pdr_override()\n",
    "\n",
    "    for stock in stocks:\n",
    "\n",
    "        data = yf.download(stock, start, end)\n",
    "        data = data.resample('D').first() \n",
    "        data = data.dropna(how='any', axis='rows')\n",
    "        \n",
    "        persistance_frame = data.copy()\n",
    "        persistance_frame = persistance_frame.drop(['Volume', 'Adj Close'], axis=1, errors='ignore')\n",
    "        persistance_frame = persistance_frame.dropna()\n",
    "        for window in [7, 30, 90]:\n",
    "            make_baselines(persistance_frame, window, stock)\n",
    "            # make_persistence(persistance_frame, window, stock)\n",
    "\n",
    "        frame, open, high, low, close = pipe.fit_transform(data)\n",
    "        inverters = [open, high, low, close]\n",
    "        _, _, future_X_7 = create_dataset(frame, 7, win_size)\n",
    "        _, _, future_X_30 = create_dataset(frame, 30, win_size)\n",
    "        _, _, future_X_90 = create_dataset(frame, 90, win_size)\n",
    "        cache = {'data':data, 'model_7':future_X_7, 'model_30':future_X_30, 'model_90':future_X_90}\n",
    "\n",
    "        prediction(models, cache, inverters, stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance(model, X_test, y_test, model_name):\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = mean_squared_error(y_test, predictions,  squared=False)\n",
    "    mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "    performance = pd.DataFrame(data={'metric': ['RMSE', 'MSE', 'MAPE', 'MAE', 'R2'], 'value': [rmse, mse, mape, mae, r2]})\n",
    "    performance.to_csv('stocks_csvs/' + model_name + '_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(stocks, future_window):\n",
    "\n",
    "    model_name = 'model_' + str(future_window)\n",
    "    set_verbosity()\n",
    "    pipe = Pipeline([('Dropper', FeatureDropper()), ('Scaler', FeatureScaler())])\n",
    "    random.shuffle(stocks)\n",
    "    cache = {}\n",
    "    persistence_frame = None\n",
    "\n",
    "    for count, stock in enumerate(stocks):\n",
    "        frame = load_frame(timeframe, stock)\n",
    "        if count == int(len(stocks) * upper):\n",
    "            persistence_frame = frame\n",
    "        elif count > int(len(stocks) * upper):\n",
    "            persistence_frame = pd.concat([persistence_frame, frame], axis=0)\n",
    "        frame, open, high, low, close = pipe.fit_transform(frame)\n",
    "        dates = frame.index[win_size:]\n",
    "        X, y, future_X = create_dataset(frame, future_window, win_size)\n",
    "        cache[stock] = [X, y,frame, open, high, low, close, future_X, dates]\n",
    "\n",
    "    make_persistence(persistance_frame, future_window)\n",
    "\n",
    "    X_train = cache.get(stocks[0])[0]\n",
    "    y_train = cache.get(stocks[0])[1]\n",
    "    X_val = cache.get(stocks[int(len(stocks) * lower)])[0]\n",
    "    y_val = cache.get(stocks[int(len(stocks) * lower)])[1]\n",
    "    X_test = cache.get(stocks[int(len(stocks) * upper)])[0]\n",
    "    y_test = cache.get(stocks[int(len(stocks) * upper)])[1]\n",
    "\n",
    "    for i in range(1, int(len(stocks) * lower)):\n",
    "        X_train = np.concatenate([X_train, cache.get(stocks[i])[0]], axis=0)\n",
    "        y_train = np.concatenate([y_train, cache.get(stocks[i])[1]], axis=0)\n",
    "\n",
    "    for i in range(int(len(stocks) * lower) + 1, int(len(stocks) * upper)):\n",
    "        X_val = np.concatenate([X_val, cache.get(stocks[i])[0]], axis=0)\n",
    "        y_val = np.concatenate([y_val, cache.get(stocks[i])[1]], axis=0)\n",
    "\n",
    "    for i in range(int(len(stocks) * upper) + 1, len(stocks)):\n",
    "        X_test = np.concatenate([X_test, cache.get(stocks[i])[0]], axis=0)\n",
    "        y_test = np.concatenate([y_test, cache.get(stocks[i])[1]], axis=0)\n",
    "\n",
    "    tuner = kt.Hyperband(kt_model, objective='mean_squared_error', max_epochs=epochs, factor=3, directory='models/kt_dir', \n",
    "            project_name=model_name, overwrite=True)\n",
    "\n",
    "    monitor = EarlyStopping(monitor='loss', min_delta=1e-4, patience=5, verbose=0, mode='auto', \n",
    "                    restore_best_weights=True)\n",
    "\n",
    "    tuner.search(cache.get(stocks[0])[0], cache.get(stocks[0])[1], verbose=1, epochs=epochs, batch_size=batch_size, callbacks=[monitor])\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    history = model.fit(X_train, y_train, verbose=1, epochs=epochs, validation_data=(X_val, y_val), callbacks=[monitor],\n",
    "                    batch_size=batch_size)\n",
    "    model.save('models/' + model_name)\n",
    "\n",
    "    test_performance(model, X_test, y_test, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "194/194 [==============================] - 8s 8ms/step\n",
      "194/194 [==============================] - 2s 4ms/step\n",
      "194/194 [==============================] - 6s 24ms/step\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "194/194 [==============================] - 1s 7ms/step\n",
      "194/194 [==============================] - 1s 4ms/step\n",
      "194/194 [==============================] - 2s 10ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m real_stocks \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAAPL\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMSFT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGOOG\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAMZN\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTSLA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNVDA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mXOM\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMETA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJNJ\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJPM\u001b[39m\u001b[39m'\u001b[39m] \n\u001b[0;32m      3\u001b[0m \u001b[39m# train_model(stocks, 7)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# train_model(stocks, 30)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# train_model(stocks, 90)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m generate_outputs(real_stocks)\n",
      "Cell \u001b[1;32mIn[31], line 15\u001b[0m, in \u001b[0;36mgenerate_outputs\u001b[1;34m(stocks)\u001b[0m\n\u001b[0;32m     11\u001b[0m yf\u001b[39m.\u001b[39mpdr_override()\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m stock \u001b[39min\u001b[39;00m stocks:\n\u001b[1;32m---> 15\u001b[0m     data \u001b[39m=\u001b[39m yf\u001b[39m.\u001b[39;49mdownload(stock, start, end)\n\u001b[0;32m     16\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mresample(\u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mfirst() \n\u001b[0;32m     17\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mdropna(how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39many\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrows\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gauld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\yfinance\\multi.py:129\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(tickers, start, end, actions, threads, ignore_tz, group_by, auto_adjust, back_adjust, repair, keepna, progress, period, show_errors, interval, prepost, proxy, rounding, timeout)\u001b[0m\n\u001b[0;32m    122\u001b[0m         _download_one_threaded(ticker, period\u001b[39m=\u001b[39mperiod, interval\u001b[39m=\u001b[39minterval,\n\u001b[0;32m    123\u001b[0m                                start\u001b[39m=\u001b[39mstart, end\u001b[39m=\u001b[39mend, prepost\u001b[39m=\u001b[39mprepost,\n\u001b[0;32m    124\u001b[0m                                actions\u001b[39m=\u001b[39mactions, auto_adjust\u001b[39m=\u001b[39mauto_adjust,\n\u001b[0;32m    125\u001b[0m                                back_adjust\u001b[39m=\u001b[39mback_adjust, repair\u001b[39m=\u001b[39mrepair, keepna\u001b[39m=\u001b[39mkeepna,\n\u001b[0;32m    126\u001b[0m                                progress\u001b[39m=\u001b[39m(progress \u001b[39mand\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m), proxy\u001b[39m=\u001b[39mproxy,\n\u001b[0;32m    127\u001b[0m                                rounding\u001b[39m=\u001b[39mrounding, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    128\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(shared\u001b[39m.\u001b[39m_DFS) \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(tickers):\n\u001b[1;32m--> 129\u001b[0m         _time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m    131\u001b[0m \u001b[39m# download synchronously\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39mfor\u001b[39;00m i, ticker \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tickers):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stocks = ['NIO', 'SQ', 'F', 'PYPL', 'GE', 'INTC', 'BA', 'AMD', 'T', 'NFLX', 'VZ', 'DIS', 'CSCO', 'PFE', 'KO']\n",
    "real_stocks = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'TSLA', 'NVDA', 'XOM', 'META', 'JNJ', 'JPM'] \n",
    "train_model(stocks, 7)\n",
    "train_model(stocks, 30)\n",
    "train_model(stocks, 90)\n",
    "generate_outputs(real_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63ac25e1e6360c0fe05ba62c52e93855fe08338e47e600dcedbcb8c5a7ec29ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
